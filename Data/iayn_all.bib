@article{kidambi2021optimism,
  title={Optimism is All You Need: Model-Based Imitation Learning From Observation Alone},
  author={Kidambi, Rahul and Chang, Jonathan and Sun, Wen},
  journal={arXiv preprint arXiv:2102.10769},
  year={2021}
}

@article{shen2020scalability,
  title={Scalability and robustness of spectral embedding: landmark diffusion is all you need},
  author={Shen, Chao and Wu, Hau-Tieng},
  journal={arXiv preprint arXiv:2001.00801},
  year={2020}
}

@article{bertasius2021space,
  title={Is Space-Time Attention All You Need for Video Understanding?},
  author={Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  journal={arXiv preprint arXiv:2102.05095},
  year={2021}
}

@article{bachsum,
  title={The sum of a geometric series is all you need!},
  author={Bach, Francis}
}

@article{de2020independent,
  title={Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?},
  author={de Witt, Christian Schroeder and Gupta, Tarun and Makoviichuk, Denys and Makoviychuk, Viktor and Torr, Philip HS and Sun, Mingfei and Whiteson, Shimon},
  journal={arXiv preprint arXiv:2011.09533},
  year={2020}
}

@inproceedings{prinzing2017friendly,
  title={Friendly superintelligent AI: all you need is love},
  author={Prinzing, Michael},
  booktitle={3rd Conference on" Philosophy and Theory of Artificial Intelligence},
  pages={288--301},
  year={2017},
  organization={Springer}
}

@article{javed2019fast,
  title={Is Fast Adaptation All You Need?},
  author={Javed, Khurram and Yao, Hengshuai and White, Martha},
  journal={arXiv preprint arXiv:1910.01705},
  year={2019}
}

@inproceedings{abdu2020adaptive,
  title={Adaptive Pooling Is All You Need: An Empirical Study on Hyperparameter-insensitive Human Action Recognition Using Wearable Sensors},
  author={Abdu-Aguye, Mubarak G and Gomaa, Walid and Makihara, Yasushi and Yagi, Yasushi},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@article{cui2021attention,
  title={Attention is all you need for general-purpose protein structure embedding},
  author={Cui, Xuefeng},
  journal={bioRxiv},
  year={2021},
  publisher={Cold Spring Harbor Laboratory}
}

@article{xiaoall,
  title={All You Need to Know about Scheduling Deep Learning Jobs},
  author={Xiao, Wencong}
}

@article{roberts2019model,
  title={Model weight theft with just noise inputs: The curious case of the petulant attacker},
  author={Roberts, Nicholas and Prabhu, Vinay Uday and McAteer, Matthew},
  journal={arXiv preprint arXiv:1912.08987},
  year={2019}
}

@article{gaogood,
  title={Good Rewards Are All You Need: Reward Learning for Efficient Reinforcement Learning in Document Summarisation},
  author={Gao, Yang and Meyer, Christian M and Mesgar, Mohsen and Gurevych, Iryna}
}

@inproceedings{ramsey2016all,
  title={All you need is the monad.. what monad was that again},
  author={Ramsey, Norman},
  booktitle={PPS Workshop},
  year={2016}
}

@article{le2020meta,
  title={Meta-Learning Is All You Need MATH 789-Mathematics of Deep Learning},
  author={Le, James},
  year={2020}
}

@article{disambiguationcross,
  title={Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!},
  author={Disambiguation, Cross-lingual Visual Verb Sense}
}

@article{isolarethinking,
  title={Rethinking Few-Shot Image Classification: A Good Embedding is All You Need?},
  author={Isola, Phillip},
  publisher={Springer}
}

@article{silva2020estimation,
  title={Estimation of the causal effect of tourism advertising campaign All You Need is Ecuador on digital exposure},
  author={Silva, Francisco Alejandro Gallegos and Alvarac{\'\i}n-Paula, Mario Eduardo and Gonz{\'a}lez, Hugo Nicol{\'a}s Acosta},
  journal={Communications in Statistics: Case Studies, Data Analysis and Applications},
  volume={6},
  number={2},
  pages={247--269},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{pedregosasufficient,
  title={Sufficient decrease is all you need},
  author={Pedregosa, Fabian}
}

@article{meinertall,
  title={All you need is a (heuristic) cue?},
  author={Meinert, Judith}
}

@article{joshiqanet,
  title={QANet: Convolutions and Attention are All you Need},
  author={Joshi, Anirudh}
}

@article{ivanov2020data,
  title={Data Movement Is All You Need: A Case Study of Transformer Networks},
  author={Ivanov, Andrei and Dryden, Nikoli and Ben-Nun, Tal and Li, Shigang and Hoefler, Torsten},
  journal={arXiv preprint arXiv:2007.00072},
  year={2020}
}

@article{namazifar2020language,
  title={Language Model is All You Need: Natural Language Understanding as Question Answering},
  author={Namazifar, Mahdi and Papangelis, Alexandros and Tur, Gokhan and Hakkani-T{\"u}r, Dilek},
  journal={arXiv preprint arXiv:2011.03023},
  year={2020}
}

@article{kostrikov2020image,
  title={Image augmentation is all you need: Regularizing deep reinforcement learning from pixels},
  author={Kostrikov, Ilya and Yarats, Denis and Fergus, Rob},
  journal={arXiv preprint arXiv:2004.13649},
  year={2020}
}

@article{eikema2020map,
  title={Is map decoding all you need? the inadequacy of the mode in neural machine translation},
  author={Eikema, Bryan and Aziz, Wilker},
  journal={arXiv preprint arXiv:2005.10283},
  year={2020}
}

@inproceedings{chen2019all,
  title={All you need is a few shifts: Designing efficient convolutional neural networks for image classification},
  author={Chen, Weijie and Xie, Di and Zhang, Yuan and Pu, Shiliang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7241--7250},
  year={2019}
}

@article{trauble2020independence,
  title={Is independence all you need? on the generalization of representations learned from correlated data},
  author={Tr{\"a}uble, Frederik and Creager, Elliot and Kilbertus, Niki and Goyal, Anirudh and Locatello, Francesco and Sch{\"o}lkopf, Bernhard and Bauer, Stefan},
  journal={arXiv preprint arXiv:2006.07886},
  year={2020}
}

@article{tamborrino2020pre,
  title={Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning},
  author={Tamborrino, Alexandre and Pellicano, Nicola and Pannier, Baptiste and Voitot, Pascal and Naudin, Louise},
  journal={arXiv preprint arXiv:2004.14074},
  year={2020}
}

@article{barnes2015azure,
  title={Azure machine learning},
  author={Barnes, Jeff},
  journal={Microsoft Azure Essentials. 1st ed, Microsoft},
  year={2015}
}

@article{uhlich2019mixed,
  title={Mixed precision dnns: All you need is a good parametrization},
  author={Uhlich, Stefan and Mauch, Lukas and Cardinaux, Fabien and Yoshiyama, Kazuki and Garcia, Javier Alonso and Tiedemann, Stephen and Kemp, Thomas and Nakamura, Akira},
  journal={arXiv preprint arXiv:1905.11452},
  year={2019}
}

@inproceedings{kalavri2018three,
  title={Three steps is all you need: fast, accurate, automatic scaling decisions for distributed streaming dataflows},
  author={Kalavri, Vasiliki and Liagouris, John and Hoffmann, Moritz and Dimitrova, Desislava and Forshaw, Matthew and Roscoe, Timothy},
  booktitle={13th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 18)},
  pages={783--798},
  year={2018}
}

@article{le2020meta,
  title={Meta-Learning Is All You Need},
  author={Le, James},
  year={2020}
}

@article{wu2021weak,
  title={Weak NAS Predictors Are All You Need},
  author={Wu, Junru and Dai, Xiyang and Chen, Dongdong and Chen, Yinpeng and Liu, Mengchen and Yu, Ye and Wang, Zhangyang and Liu, Zicheng and Chen, Mei and Yuan, Lu},
  journal={arXiv preprint arXiv:2102.10490},
  year={2021}
}

@article{zhu2021node,
  title={Node Proximity Is All You Need: Unified Structural and Positional Node and Graph Embedding},
  author={Zhu, Jing and Lu, Xingyu and Heimann, Mark and Koutra, Danai},
  journal={arXiv preprint arXiv:2102.13582},
  year={2021}
}

@article{bilkhu2019attention,
  title={Attention is all you need for Videos: Self-attention based Video Summarization using Universal Transformers},
  author={Bilkhu, Manjot and Wang, Siyang and Dobhal, Tushar},
  journal={arXiv preprint arXiv:1906.02792},
  year={2019}
}

@inproceedings{prasad2020robust,
  title={A Robust Univariate Mean Estimator is All You Need},
  author={Prasad, Adarsh and Balakrishnan, Sivaraman and Ravikumar, Pradeep},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4034--4044},
  year={2020},
  organization={PMLR}
}

@article{kumar2020one,
  title={One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL},
  author={Kumar, Saurabh and Kumar, Aviral and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2010.14484},
  year={2020}
}

@article{ivanov2020data,
  title={Data Movement Is All You Need: A Case Study on Optimizing Transformers},
  author={Ivanov, Andrei and Dryden, Nikoli and Ben-Nun, Tal and Li, Shigang and Hoefler, Torsten},
  journal={arXiv e-prints},
  pages={arXiv--2007},
  year={2020}
}

@article{ren2021generative,
  title={Do Generative Models Know Disentanglement? Contrastive Learning is All You Need},
  author={Ren, Xuanchi and Yang, Tao and Wang, Yuwang and Zeng, Wenjun},
  journal={arXiv preprint arXiv:2102.10543},
  year={2021}
}

@article{boudiaf2020few,
  title={Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?},
  author={Boudiaf, Malik and Kervadec, Hoel and Masud, Ziko Imtiaz and Piantanida, Pablo and Ayed, Ismail Ben and Dolz, Jose},
  journal={arXiv preprint arXiv:2012.06166},
  year={2020}
}

@article{blonde2020lipschitzness,
  title={Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning},
  author={Blond{\'e}, Lionel and Strasser, Pablo and Kalousis, Alexandros},
  journal={arXiv preprint arXiv:2006.16785},
  year={2020}
}
@article{mishkin2015all,
  title={All you need is a good init},
  author={Mishkin, Dmytro and Matas, Jiri},
  journal={arXiv preprint arXiv:1511.06422},
  year={2015}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@article{chollet2017limitations,
  title={The limitations of deep learning},
  author={Chollet, Francois},
  journal={Deep Learning With Python},
  year={2017},
  publisher={Manning Publications}
}

@article{tran2020all,
  title={All You Need is a Good Functional Prior for Bayesian Deep Learning},
  author={Tran, Ba-Hien and Rossi, Simone and Milios, Dimitrios and Filippone, Maurizio},
  journal={arXiv preprint arXiv:2011.12829},
  year={2020}
}

@article{chen2017cnn,
  title={CNN is all you need},
  author={Chen, Qiming and Wu, Ren},
  journal={arXiv preprint arXiv:1712.09662},
  year={2017}
}

@article{bachlechner2020rezero,
  title={Rezero is all you need: Fast convergence at large depth},
  author={Bachlechner, Thomas and Majumder, Bodhisattwa Prasad and Mao, Huanru Henry and Cottrell, Garrison W and McAuley, Julian},
  journal={arXiv preprint arXiv:2003.04887},
  year={2020}
}

@inproceedings{xie2017all,
  title={All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation},
  author={Xie, Di and Xiong, Jiang and Pu, Shiliang},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6176--6185},
  year={2017}
}

@inproceedings{guo2019depthwise,
  title={Depthwise convolution is all you need for learning multiple visual domains},
  author={Guo, Yunhui and Li, Yandong and Wang, Liqiang and Rosing, Tajana},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={8368--8375},
  year={2019}
}

@article{li2020deepergcn,
  title={Deepergcn: All you need to train deeper gcns},
  author={Li, Guohao and Xiong, Chenxin and Thabet, Ali and Ghanem, Bernard},
  journal={arXiv preprint arXiv:2006.07739},
  year={2020}
}

@article{jeddi2020simple,
  title={A Simple Fine-tuning Is All You Need: Towards Robust Deep Learning Via Adversarial Fine-tuning},
  author={Jeddi, Ahmadreza and Shafiee, Mohammad Javad and Wong, Alexander},
  journal={arXiv preprint arXiv:2012.13628},
  year={2020}
}

@inproceedings{grondahl2018all,
  title={All You Need is" Love" Evading Hate Speech Detection},
  author={Gr{\"o}ndahl, Tommi and Pajola, Luca and Juuti, Mika and Conti, Mauro and Asokan, N},
  booktitle={Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security},
  pages={2--12},
  year={2018}
}

@article{hu2021transformer,
  title={Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer},
  author={Hu, Ronghang and Singh, Amanpreet},
  journal={arXiv preprint arXiv:2102.10772},
  year={2021}
}

@inproceedings{wang2020all,
  title={All you need is boundary: Toward arbitrary-shaped text spotting},
  author={Wang, Hao and Lu, Pu and Zhang, Hui and Yang, Mingkun and Bai, Xiang and Xu, Yongchao and He, Mengchao and Wang, Yongpan and Liu, Wenyu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={07},
  pages={12160--12167},
  year={2020}
}

@article{subakan2020attention,
  title={Attention is All You Need in Speech Separation},
  author={Subakan, Cem and Ravanelli, Mirco and Cornell, Samuele and Bronzi, Mirko and Zhong, Jianyuan},
  journal={arXiv preprint arXiv:2010.13154},
  year={2020}
}

@article{sharp2020diffusion,
  title={Diffusion is All You Need for Learning on Surfaces},
  author={Sharp, Nicholas and Attaiki, Souhaib and Crane, Keenan and Ovsjanikov, Maks},
  journal={arXiv preprint arXiv:2012.00888},
  year={2020}
}

@article{sharp2020diffusion,
  title={Diffusion is All You Need for Learning on Surfaces},
  author={Sharp, Nicholas and Attaiki, Souhaib and Crane, Keenan and Ovsjanikov, Maks},
  journal={arXiv preprint arXiv:2012.00888},
  year={2020}
}


@inproceedings{o2019deep,
  title={Deep learning vs. traditional computer vision},
  author={O’Mahony, Niall and Campbell, Sean and Carvalho, Anderson and Harapanahalli, Suman and Hernandez, Gustavo Velasco and Krpalkova, Lenka and Riordan, Daniel and Walsh, Joseph},
  booktitle={Science and Information Conference},
  pages={128--144},
  year={2019},
  organization={Springer}
}

@inproceedings{chaudhury20203d,
  title={3D Plant Phenotyping: All You Need is Labelled Point Cloud Data},
  author={Chaudhury, Ayan and Boudon, Fr{\'e}d{\'e}ric and Godin, Christophe},
  booktitle={European Conference on Computer Vision},
  pages={244--260},
  year={2020},
  organization={Springer}
}

@article{lu2020evolution,
  title={Evolution Is All You Need: Phylogenetic Augmentation for Contrastive Learning},
  author={Lu, Amy X and Lu, Alex X and Moses, Alan},
  journal={arXiv preprint arXiv:2012.13475},
  year={2020}
}

@inproceedings{kato2019gans,
  title={Gans-based clothes design: Pattern maker is all you need to design clothing},
  author={Kato, Natsumi and Osone, Hiroyuki and Oomori, Kotaro and Ooi, Chun Wei and Ochiai, Yoichi},
  booktitle={Proceedings of the 10th Augmented Human International Conference 2019},
  pages={1--7},
  year={2019}
}
@misc{dong2021attention,
    title   = {Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth}, 
    author  = {Yihe Dong and Jean-Baptiste Cordonnier and Andreas Loukas},
    year    = {2021},
    eprint  = {2103.03404}
}